{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import ast\n",
    "import os\n",
    "import argparse\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path added to sys.path: e:\\Tasnim\\SonyCSL\\fund_sustainable_activity_tracking_main\\fund_sustainable_activity_tracking-main\n"
     ]
    }
   ],
   "source": [
    "base_path = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(base_path)\n",
    "\n",
    "print(\"Base path added to sys.path:\", base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparametr\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dropout_prob = 0.2\n",
    "batch_size = 16\n",
    "test_batch_size = 1 # must be fixed to 1\n",
    "epochs= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(custom_args=None):\n",
    "    \"\"\"\n",
    "    make parser to get parameters\n",
    "    \"\"\"\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='VAE',\n",
    "        description='using VAE for 768 dimensional data')\n",
    "    \n",
    "    parser.add_argument('--filename', type = str, default = 'data/prospectus_investment_objective.txt', help='source txt file')\n",
    "    parser.add_argument('--model_savefolder', type = str, default = 'model/state', help='best model save folder')\n",
    "    \n",
    "    # Parse arguments\n",
    "    if custom_args:\n",
    "        return parser.parse_args(custom_args)\n",
    "    else:\n",
    "        return parser.parse_args()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        \n",
    "        self.vector = dataframe[4]\n",
    "        self.id = dataframe[0]\n",
    "        self.name = dataframe[1]\n",
    "        self.type_no = dataframe[2]\n",
    "        self.sentence = dataframe[3]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        vector_data = np.array(ast.literal_eval(self.vector[index])).astype(np.float32)\n",
    "        criteria_no = int(self.type_no[index])\n",
    "        return vector_data, self.id[index], self.name[index], criteria_no, self.sentence[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.vector)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=768, hidden_dim=[600, 500, 400, 300, 200, 100, 50], latent_dim = 2, device=device):\n",
    "        \n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim[0]),\n",
    "            nn.BatchNorm1d(hidden_dim[0]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[0], hidden_dim[1]),\n",
    "            nn.BatchNorm1d(hidden_dim[1]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[1], hidden_dim[2]),\n",
    "            nn.BatchNorm1d(hidden_dim[2]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[2], hidden_dim[3]),\n",
    "            nn.BatchNorm1d(hidden_dim[3]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[3], hidden_dim[4]),\n",
    "            nn.BatchNorm1d(hidden_dim[4]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[4], hidden_dim[5]),\n",
    "            nn.BatchNorm1d(hidden_dim[5]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[5], hidden_dim[6]),\n",
    "            nn.BatchNorm1d(hidden_dim[6]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[6], latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        \n",
    "        # latent mean and variance \n",
    "        self.mean_layer = nn.Linear(latent_dim, 2)\n",
    "        self.logvar_layer = nn.Linear(latent_dim, 2)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, latent_dim),\n",
    "            nn.BatchNorm1d(latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(latent_dim, hidden_dim[6]),\n",
    "            nn.BatchNorm1d(hidden_dim[6]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[6], hidden_dim[5]),\n",
    "            nn.BatchNorm1d(hidden_dim[5]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[5], hidden_dim[4]),\n",
    "            nn.BatchNorm1d(hidden_dim[4]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[4], hidden_dim[3]),\n",
    "            nn.BatchNorm1d(hidden_dim[3]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[3], hidden_dim[2]),\n",
    "            nn.BatchNorm1d(hidden_dim[2]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[2], hidden_dim[1]),\n",
    "            nn.BatchNorm1d(hidden_dim[1]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[1], hidden_dim[0]),\n",
    "            nn.BatchNorm1d(hidden_dim[0]),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(dropout_prob), \n",
    "            nn.Linear(hidden_dim[0], input_dim),\n",
    "            # nn.Sigmoid()\n",
    "            )\n",
    "     \n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        mean, logvar = self.mean_layer(x), self.logvar_layer(x)\n",
    "        return mean, logvar, x\n",
    "\n",
    "    def reparameterization(self, mean, var):\n",
    "        epsilon = torch.randn_like(var).to(device)      \n",
    "        z = mean + var*epsilon\n",
    "        return z\n",
    "\n",
    "    def decode(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, logvar, _ = self.encode(x)\n",
    "        z = self.reparameterization(mean, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mean, logvar\n",
    "        \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(x, x_hat, mean, log_var):\n",
    "    \n",
    "    \n",
    "    KLD = - 0.5 * torch.sum(1+ log_var - mean.pow(2) - log_var.exp())\n",
    "    reconstruction_loss_fn = nn.MSELoss()\n",
    "    reconstruction_loss = reconstruction_loss_fn(x_hat, x)\n",
    "    wr = 1\n",
    "    wk = 10\n",
    "    \n",
    "    loss = wr * reconstruction_loss + wk * KLD\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, optimizer, epochs, device, batch_size, train_loader, test_loader, x_dim=768):\n",
    "    model.train()\n",
    "    train_loss_list = []\n",
    "    test_loss_list = []\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        for batch_idx, xf in enumerate(train_loader):\n",
    "            x = xf[0].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Train Loss: \", train_loss/(batch_idx*batch_size))\n",
    "        \n",
    "        current_train_loss = train_loss/(batch_idx*batch_size)\n",
    "        train_loss_list.append(current_train_loss)\n",
    "        \n",
    "        if current_train_loss == min(train_loss_list):\n",
    "            model_savename = 'vae_model_best.pth'\n",
    "            \n",
    "            torch.save(model, os.path.join(base_path, args.model_savefolder, model_savename))\n",
    "            \n",
    "        model.eval()        \n",
    "        test_loss = 0\n",
    "        test_batch_size = 1\n",
    "        for batch_idx, xf in enumerate(test_loader):\n",
    "            x = xf[0]\n",
    "            x = x.view(test_batch_size, x_dim).to(device)\n",
    "            x_hat, mean, log_var = model(x)\n",
    "            loss = loss_function(x, x_hat, mean, log_var)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "\n",
    "        print(\"\\tEpoch\", epoch + 1, \"\\tAverage Test Loss: \", test_loss/(batch_idx*test_batch_size))\n",
    "        test_loss_list.append(test_loss/(batch_idx*test_batch_size))\n",
    "        \n",
    "        epoch_count = range(1, len(train_loss_list) + 1)\n",
    "\n",
    "        # Visualize loss history\n",
    "        '''\n",
    "        plt.plot(epoch_count, train_loss_list, 'r-')\n",
    "        plt.plot(epoch_count, test_loss_list, 'b-')\n",
    "        plt.legend(['Training Loss', 'Test Loss'])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.savefig('VAE train_and test_loss_plot', bbox_inches='tight', dpi =200)\n",
    "        # plt.show()\n",
    "        '''\n",
    "    return train_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    custom_args = ['--filename', 'data/prospectus_investment_objective.txt', '--model_savefolder', 'model/state']  # args\n",
    "    args = get_args(custom_args)\n",
    "\n",
    "    basedir = os.path.dirname(args.filename)\n",
    "    print('basedir: ', basedir)\n",
    "    basename = os.path.basename(args.filename)\n",
    "    \n",
    "    if not os.path.exists(os.path.join(base_path, args.model_savefolder)):\n",
    "        os.makedirs(os.path.join(base_path, args.model_savefolder))\n",
    "        \n",
    "    df = pd.read_csv(os.path.join(base_path, args.filename), sep=\"\\t\", header=None) \n",
    "    dataset = CustomDataset(dataframe=df)\n",
    "\n",
    "    train_size = int(0.8 * len(dataset)) \n",
    "    test_size = len(dataset) - train_size \n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=test_batch_size, shuffle=False)\n",
    "    \n",
    "    model = VAE().to(device)\n",
    "   \n",
    "    optimizer = Adam(model.parameters(), lr=1e-7)\n",
    "    \n",
    "    train(args, model, optimizer, epochs, device, batch_size, train_loader, test_loader)\n",
    "    print('training finished')\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basedir:  data\n",
      "\tEpoch 1 \tAverage Train Loss:  4.75022909358928\n",
      "\tEpoch 1 \tAverage Test Loss:  1.2634422229685622\n",
      "\tEpoch 2 \tAverage Train Loss:  1.0399284933742723\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 27\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m VAE()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-7\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining finished\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 39\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, optimizer, epochs, device, batch_size, train_loader, test_loader, x_dim)\u001b[0m\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m xf[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     38\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(test_batch_size, x_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 39\u001b[0m x_hat, mean, log_var \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(x, x_hat, mean, log_var)\n\u001b[0;32m     41\u001b[0m test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\hand_obj_processing\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 100\u001b[0m, in \u001b[0;36mVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     98\u001b[0m mean, logvar, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m     99\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterization(mean, logvar)\n\u001b[1;32m--> 100\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_hat, mean, logvar\n",
      "Cell \u001b[1;32mIn[16], line 95\u001b[0m, in \u001b[0;36mVAE.decode\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\hand_obj_processing\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\hand_obj_processing\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\hand_obj_processing\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Anaconda3\\envs\\hand_obj_processing\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hand_obj_processing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
